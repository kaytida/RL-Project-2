{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from gymnasium.envs.classic_control.pendulum import PendulumEnv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return ((x + np.pi) % (2 * np.pi)) - np.pi\n",
    "\n",
    "class MyPendulumEnv(PendulumEnv):\n",
    "    def __init__(self, render_mode: Optional[str] = None, g=10.0):\n",
    "        super().__init__(render_mode, g)\n",
    "        self.m = 1.5\n",
    "        self.l = 0.8\n",
    "        self.b = 0.2\n",
    "    \n",
    "    def step(self, u):\n",
    "        th, thdot = self.state  # th := theta\n",
    "\n",
    "        g = self.g\n",
    "        m = self.m\n",
    "        l = self.l\n",
    "        b = self.b\n",
    "        dt = self.dt\n",
    "\n",
    "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
    "        self.last_u = u  # for rendering\n",
    "        costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)\n",
    "\n",
    "        newthdot = thdot + (u / (m * l**2) - g / l * np.sin(th) - b * thdot / (m * l**2)) * dt\n",
    "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
    "        newth = th + newthdot * dt\n",
    "\n",
    "        terminated = True if np.abs(angle_normalize(th) - np.pi) < 0.1 else False\n",
    "\n",
    "        self.state = np.array([newth, newthdot])\n",
    "        return self._get_obs(), -costs, terminated, False, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf videos-sb videos-my ppo-pendulum-sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "\n",
    "register(\"Pendulum-v2\", entry_point=MyPendulumEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/homebrew/bin/ffmpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "from datetime import datetime\n",
    "\n",
    "vec_env = VecVideoRecorder(venv=make_vec_env(\"Pendulum-v2\", n_envs=1), video_folder=\"./videos-sb\", name_prefix=\"pendulum\", record_video_trigger=lambda x: x % 10240 == 0)\n",
    "model = PPO(\"MlpPolicy\", vec_env, n_steps=1024, verbose=1, tensorboard_log=\"./ppo-pendulum-sb/\", device=\"mps\")\n",
    "model.learn(total_timesteps=20480, tb_log_name=\"run-\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "model.save(\"ppo_pendulum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "\n",
    "vec_env = VecVideoRecorder(venv=make_vec_env(\"Pendulum-v2\", n_envs=1), video_folder=\"./videos-sb\", name_prefix=\"pendulum\", record_video_trigger=lambda x: x % 10240 == 0)\n",
    "\n",
    "model = PPO.load(\"ppo_pendulum\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "vec_env.start_video_recorder()\n",
    "for _ in range(1024):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(mode='rgb_array')\n",
    "vec_env.close_video_recorder()\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./ppo-pendulum-sb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video src=\"videos-sb/pendulum-step-10240-to-step-10440.mp4\" controls autoplay loop />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, env: MyPendulumEnv):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            self.layer_init(nn.Linear(np.prod(env.observation_space.shape), 64)),\n",
    "            nn.Tanh(),\n",
    "            self.layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            self.layer_init(nn.Linear(64, 1), std=1.0)\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            self.layer_init(nn.Linear(np.prod(env.observation_space.shape), 64)),\n",
    "            nn.Tanh(),\n",
    "            self.layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            self.layer_init(nn.Linear(64, np.prod(env.action_space.shape)), std=0.01)\n",
    "        )\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(env.action_space.shape)))\n",
    "    \n",
    "    def layer_init(self, layer, std=np.sqrt(2), bias=0):\n",
    "        nn.init.orthogonal_(layer.weight, std)\n",
    "        nn.init.constant_(layer.bias, bias)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        action_mean = self.actor(x)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)\n",
    "\n",
    "@dataclass\n",
    "class PPOConfig:\n",
    "    n_steps: int = 2048 # Number of steps to run for each environment per update\n",
    "    learning_rate: float = 3e-4 # Learning rate\n",
    "    eps: float = 1e-5 # Adam epsilon\n",
    "    gamma: float = 0.99 # Discount factor\n",
    "    num_envs: int = 1 # Number of environments\n",
    "    num_steps: int = 2048 # Number of steps\n",
    "    gae_lambda: float = 0.95 # Lambda for GAE\n",
    "    clip_coef: float = 0.2 # Clip parameter for PPO\n",
    "    vf_coef: float = 0.5 # Value function coefficient\n",
    "    ent_coef: float = 0.01 # Entropy coefficient\n",
    "    num_minibatches: int = 64 # Number of minibatches\n",
    "    total_timesteps: int = 102400 # Total number of steps\n",
    "    batch_size: int = 2048 # Batch size\n",
    "    minibatch_size = 32 # Minibatch size\n",
    "    update_epochs: int = 10 # Number of epochs\n",
    "    max_grad_norm: float = 0.5 # Maximum gradient norm\n",
    "\n",
    "class MyPPO:\n",
    "    def __init__(self, n_epochs, env: MyPendulumEnv):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.config = PPOConfig()\n",
    "        self.rewards = []\n",
    "    \n",
    "    def learn(self):\n",
    "        device = self.device\n",
    "        agent = ActorCritic(self.env).to(device)\n",
    "        print(agent)\n",
    "        optimizer = optim.Adam(agent.parameters(), lr=self.config.learning_rate, eps=self.config.eps)\n",
    "\n",
    "        obs = torch.zeros((self.config.num_steps, self.config.num_envs) + self.env.observation_space.shape).to(self.device)\n",
    "        actions = torch.zeros((self.config.num_steps, self.config.num_envs) + self.env.action_space.shape).to(device)\n",
    "        logprobs = torch.zeros((self.config.num_steps, self.config.num_envs)).to(device)\n",
    "        rewards = torch.zeros((self.config.num_steps, self.config.num_envs)).to(device)\n",
    "        dones = torch.zeros((self.config.num_steps, self.config.num_envs)).to(device)\n",
    "        values = torch.zeros((self.config.num_steps, self.config.num_envs)).to(device)\n",
    "\n",
    "        global_step = 0\n",
    "        next_obs = torch.Tensor(self.env.reset()).to(device)\n",
    "        next_done = torch.zeros(self.config.num_envs).to(device)\n",
    "        num_updates = int(self.config.total_timesteps // self.config.batch_size)\n",
    "\n",
    "        for update in range(1, num_updates + 1):\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = frac * self.config.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "            for step in range(0, self.config.num_steps):\n",
    "                global_step += 1 * self.config.num_envs\n",
    "                obs[step] = next_obs\n",
    "                dones[step] = next_done\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                    values[step] = value.flatten()\n",
    "                actions[step] = action\n",
    "                logprobs[step] = logprob\n",
    "\n",
    "                next_obs, reward, done, _ = self.env.step(action.cpu().numpy())\n",
    "                rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "                next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "                self.rewards.append(reward)\n",
    "\n",
    "                if done:\n",
    "                    self.env.reset()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "                advantages = torch.zeros_like(rewards).to(device)\n",
    "                lastgaelam = 0\n",
    "                for t in reversed(range(self.config.num_steps)):\n",
    "                    if t == self.config.num_steps - 1:\n",
    "                        nextnonterminal = 1.0 - next_done\n",
    "                        nextvalues = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - dones[t + 1]\n",
    "                        nextvalues = values[t + 1]\n",
    "                    delta = rewards[t] + self.config.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                    advantages[t] = lastgaelam = delta + self.config.gamma * self.config.gae_lambda * nextnonterminal * lastgaelam\n",
    "                returns = advantages + values\n",
    "\n",
    "            b_obs = obs.reshape((-1,) + self.env.observation_space.shape)\n",
    "            b_logprobs = logprobs.reshape(-1)\n",
    "            b_actions = actions.reshape((-1,) + self.env.action_space.shape)\n",
    "            b_advantages = advantages.reshape(-1)\n",
    "            b_returns = returns.reshape(-1)\n",
    "            b_values = values.reshape(-1)\n",
    "\n",
    "            b_inds = np.arange(self.config.batch_size)\n",
    "            clipfracs = []\n",
    "            for _ in range(self.config.update_epochs):\n",
    "                np.random.shuffle(b_inds)\n",
    "                for start in range(0, self.config.batch_size, self.config.minibatch_size):\n",
    "                    end = start + self.config.minibatch_size\n",
    "                    mb_inds = b_inds[start:end]\n",
    "\n",
    "                    _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
    "                    logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                    ratio = logratio.exp()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        clipfracs += [((ratio - 1.0).abs() > self.config.clip_coef).float().mean().item()]\n",
    "\n",
    "                    mb_advantages = b_advantages[mb_inds]\n",
    "\n",
    "                    pg_loss1 = -mb_advantages * ratio\n",
    "                    pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - self.config.clip_coef, 1 + self.config.clip_coef)\n",
    "                    pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                    newvalue = newvalue.view(-1)\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -self.config.clip_coef,\n",
    "                        self.config.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                    entropy_loss = entropy.mean()\n",
    "                    loss = pg_loss - self.config.ent_coef * entropy_loss + v_loss * self.config.vf_coef\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(agent.parameters(), self.config.max_grad_norm)\n",
    "                    optimizer.step()\n",
    "    \n",
    "    def plot_rewards(self):\n",
    "        print(f\"rewards={self.rewards}\")\n",
    "        plt.plot(self.rewards)\n",
    "        plt.title(\"Pendulum-v2 rewards\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "\n",
    "\n",
    "vec_env = VecVideoRecorder(venv=make_vec_env(\"Pendulum-v2\", n_envs=1), video_folder=\"./videos-my\", name_prefix=\"pendulum\", record_video_trigger=lambda x: x % 10240 == 0)\n",
    "model = MyPPO(n_epochs=4, env=vec_env)\n",
    "model.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_rewards()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
